{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import music21 as m21\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_DURATIONS = [\n",
    "    0.25, # Semicorchea/Step  (ignoramos semicorchea con punto porque usa fusas)\n",
    "    0.5,  # Corchea\n",
    "    0.75, # Corchea con punto\n",
    "    1,    # Negra\n",
    "    1.25, # Negra ligada a semi-corchea\n",
    "    1.5,  # Negra con punto\n",
    "    1.75, # Negra con punto ligada a semi-corchea\n",
    "    2,    # Blanca \n",
    "    2.25, # Blanca ligada a semi-corchea\n",
    "    2.5,  # Blanca ligada a corchea\n",
    "    2.75, # Blanca ligada a corchea con punto\n",
    "    3,    # Blanca con punto\n",
    "    3.25, # Blanca con punto ligada a semi-corchea\n",
    "    3.5,  # Blanca con punto ligada a corchea\n",
    "    3.75, # Blanca con punto ligada a corchea con punto\n",
    "    4     # Redonda\n",
    "]\n",
    "STEP_DURATION = 0.25   # In relation to a quarter note (basically a 16th note)\n",
    "\n",
    "# Pre-processing\n",
    "SEQUENCE_LENGTH = 64\n",
    "DELIMITER_SYMBOL = '/'\n",
    "NON_MIDI_SYMBOLS = ['r', '_', DELIMITER_SYMBOL]\n",
    "NUMBER_OF_MIDI_VALUES = 128\n",
    "\n",
    "# Paths\n",
    "RAW_DATASET_PATH = 'data/raw_dataset'\n",
    "PREPROCESSED_DATASET_DESTINATION = 'data/preprocessed_dataset/individual_scores'\n",
    "MERGED_DATASET_DESTINATION = 'data/preprocessed_dataset/merged_preprocessed_dataset.txt'\n",
    "LOOKUP_TABLE_DESTINATION = 'data/lookup_table.json'\n",
    "DEFAULT_MODEL_PATH = 'model.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "\n",
    "    scores = load_training_data()\n",
    "\n",
    "    # Create and save time series for individual scores\n",
    "    preprocess_individual_scores(scores)\n",
    "\n",
    "    # Create single file dataset by merging all time series scores\n",
    "    create_merged_dataset()\n",
    "\n",
    "def load_training_data():\n",
    "\n",
    "    scores = []\n",
    "    for path, _, files in os.walk(RAW_DATASET_PATH):\n",
    "        for file in files:\n",
    "            if file[-4:] == \".krn\":\n",
    "                score = m21.converter.parse(os.path.join(path, file))\n",
    "                scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def preprocess_individual_scores(scores):\n",
    "\n",
    "    for i, score in enumerate(scores):\n",
    "        \n",
    "        # Filter score with unsupported durations\n",
    "        duration_complaint = check_durations(score)\n",
    "        if not duration_complaint: continue\n",
    "\n",
    "        # Transpose score to Cmaj/Amin keys\n",
    "        transposed_score = transpose_music_to_CA(score)\n",
    "\n",
    "        # Encode score in time-series representation\n",
    "        encoded_score = encode_music(transposed_score)\n",
    "        \n",
    "        preprocessed_score_path = f'{PREPROCESSED_DATASET_DESTINATION}/{i}-preprocessed_score.txt'\n",
    "        with open(preprocessed_score_path, 'w') as fp:\n",
    "            fp.write(encoded_score)\n",
    "\n",
    "def check_durations(score):\n",
    "\n",
    "    for musical_event in score.flat.notesAndRests:\n",
    "        if musical_event.duration.quarterLength not in SUPPORTED_DURATIONS: return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def transpose_music_to_CA(score, key='n/a', mode='n/a'):\n",
    "    \n",
    "    # No user input (training)\n",
    "    if key == 'n/a':\n",
    "    \n",
    "        # Get key by metadata\n",
    "        parts = score.getElementsByClass(m21.stream.Part)\n",
    "        measures_part0 = parts[0].getElementsByClass(m21.stream.Measure) \n",
    "        key = measures_part0[0][4] # Here is where key resides if in metadata\n",
    "\n",
    "        if not isinstance(key, m21.key.Key):\n",
    "            key = score.analyze(\"key\") \n",
    "\n",
    "        mode = key.mode\n",
    "\n",
    "    # Get interval\n",
    "    if mode=='major':\n",
    "        interval = m21.interval.Interval(get_pitch(key), m21.pitch.Pitch('C'))  \n",
    "    elif mode=='minor': \n",
    "        interval = m21.interval.Interval(get_pitch(key), m21.pitch.Pitch('A'))  \n",
    "    else: # Modal music, no need to transpose (should not happen or very marginal case)\n",
    "        interval = m21.interval.Interval(\"P1\") \n",
    "\n",
    "    transposed_score = score.transpose(interval)\n",
    "    return transposed_score\n",
    "\n",
    "def get_pitch(key):\n",
    "\n",
    "    if isinstance(key, m21.key.Key): return key.tonic\n",
    "    return m21.pitch.Pitch(key)\n",
    "\n",
    "def encode_music(transposed_score):\n",
    "\n",
    "    # Encode seed in time series string\n",
    "    encoded_score = []\n",
    "\n",
    "    for musical_event in transposed_score.flat.notesAndRests:\n",
    "\n",
    "        if isinstance(musical_event, m21.note.Note):\n",
    "            event_type = musical_event.pitch.midi  \n",
    "        else:\n",
    "            event_type = \"r\"\n",
    "\n",
    "        # Get event duration\n",
    "        num_of_steps = int(musical_event.duration.quarterLength / STEP_DURATION) # Event duration\n",
    "\n",
    "        # Encode event and it's duration\n",
    "        encoded_score += [event_type] + [\"_\"] * (num_of_steps - 1)\n",
    "\n",
    "\n",
    "    # Make string out of whole list \n",
    "    encoded_score = \" \".join(map(str, encoded_score))\n",
    "    return encoded_score\n",
    "\n",
    "def create_merged_dataset():\n",
    "    merged_timeseries = ''\n",
    "    song_separator = (DELIMITER_SYMBOL + ' ') * SEQUENCE_LENGTH\n",
    "\n",
    "    for path, _, files in os.walk(PREPROCESSED_DATASET_DESTINATION):\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(path, file)\n",
    "\n",
    "            with open(file_path, 'r') as fp: \n",
    "                single_timeseries_score = fp.read()\n",
    "\n",
    "            merged_timeseries = merged_timeseries + single_timeseries_score + \" \" + song_separator\n",
    "\n",
    "\n",
    "    with open(MERGED_DATASET_DESTINATION, 'w') as fp:\n",
    "        fp.write(merged_timeseries)\n",
    "\n",
    "preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None, 131)]       0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, None, 256)         397312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 131)               33667     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 956,291\n",
      "Trainable params: 956,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "12/12 [==============================] - 25s 858ms/step - loss: 2.9921 - accuracy: 0.5250\n"
     ]
    }
   ],
   "source": [
    "NUM_UNITS = [256, 256]\n",
    "LOSS = \"sparse_categorical_crossentropy\"\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train():\n",
    "\n",
    "    inputs, targets = generate_training_sequences()\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    model.fit(inputs, targets, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model.save(DEFAULT_MODEL_PATH)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    output_units = get_vocabulary_size()\n",
    "\n",
    "    input = keras.layers.Input(shape=(None, output_units))\n",
    "    x = keras.layers.LSTM(NUM_UNITS[0], return_sequences=True)(input)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = keras.layers.LSTM(NUM_UNITS[1])(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    output = keras.layers.Dense(output_units, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(input, output)\n",
    "\n",
    "    model.compile(loss=LOSS,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_training_sequences():\n",
    "    sequence_length = SEQUENCE_LENGTH\n",
    "    with open(MERGED_DATASET_DESTINATION) as f:\n",
    "      songs = f.read()\n",
    "    int_songs = convert_songs_to_int(songs)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    num_sequences = len(int_songs) - sequence_length\n",
    "    for i in range(num_sequences):\n",
    "        inputs.append(int_songs[i:i+sequence_length])\n",
    "        targets.append(int_songs[i+sequence_length])\n",
    "\n",
    "    vocabulary_size = get_vocabulary_size()\n",
    "    inputs = keras.utils.to_categorical(inputs, num_classes=vocabulary_size)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "def convert_songs_to_int(songs):\n",
    "    with open(LOOKUP_TABLE_DESTINATION, \"r\") as fp:\n",
    "        mappings = json.load(fp)\n",
    "\n",
    "    songs = songs.split()\n",
    "\n",
    "    int_songs = [mappings[x] for x in songs]\n",
    "\n",
    "    return int_songs\n",
    "\n",
    "def get_vocabulary_size():\n",
    "  with open(LOOKUP_TABLE_DESTINATION, \"r\") as fp:\n",
    "    mappings = json.load(fp)\n",
    "  return len(mappings.keys())\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de melodías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model =  keras.models.load_model(DEFAULT_MODEL_PATH)\n",
    "    mappings = json.load(open(LOOKUP_TABLE_DESTINATION))\n",
    "    start_symbols = ['/'] * SEQUENCE_LENGTH\n",
    "    return model, mappings, start_symbols\n",
    "\n",
    "model, mappings, start_symbols = load_model()\n",
    "\n",
    "def generate_melody( \n",
    "      seed, \n",
    "      num_steps=500, \n",
    "      max_sequence_length=SEQUENCE_LENGTH, \n",
    "      temperature=1.0,\n",
    "      model=model,\n",
    "      mappings=mappings,\n",
    "      start_symbols=start_symbols\n",
    "    ):\n",
    "        seed = seed.split()\n",
    "        melody = seed\n",
    "        seed = start_symbols + seed\n",
    "        seed = [mappings[symbol] for symbol in seed]\n",
    "\n",
    "        for _ in range(num_steps):\n",
    "\n",
    "            seed = seed[-max_sequence_length:]\n",
    "\n",
    "            onehot = keras.utils.to_categorical(seed, num_classes=len(mappings))\n",
    "            \n",
    "            onehot = onehot[np.newaxis, ...]\n",
    "\n",
    "            probabilities = model.predict(onehot)[0]\n",
    "\n",
    "            output_int = sample_with_temperature(probabilities, temperature)\n",
    "\n",
    "            seed.append(output_int)\n",
    "\n",
    "            output_symbol = [k for k, v in mappings.items() if v == output_int][0]\n",
    "\n",
    "            # check whether we're at the end of a melody\n",
    "            if output_symbol == \"/\":\n",
    "                break\n",
    "\n",
    "            melody.append(output_symbol)\n",
    "\n",
    "        return melody\n",
    "\n",
    "def sample_with_temperature(probabilites, temperature):\n",
    "\n",
    "        predictions = np.log(probabilites) / temperature\n",
    "        probabilites = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "\n",
    "        choices = range(len(probabilites))\n",
    "        index = np.random.choice(choices, p=probabilites)\n",
    "\n",
    "        return index\n",
    "\n",
    "\n",
    "def save_melody(\n",
    "      melody,\n",
    "      step_duration=0.25,\n",
    "      format=\"midi\",\n",
    "      file_name=\"mel.mid\",\n",
    "      key=\"C\",\n",
    "      mode=\"major\",\n",
    "      tempo=120\n",
    "    ):\n",
    "        stream = m21.stream.Stream()\n",
    "\n",
    "        start_symbol = None\n",
    "        step_counter = 1\n",
    "\n",
    "        for i, symbol in enumerate(melody):\n",
    "\n",
    "            # handle case in which we have a note/rest and its not the end of the melody\n",
    "            if symbol != \"_\" or i + 1 == len(melody):\n",
    "\n",
    "                # dealing with note/rest beyond the first one\n",
    "                if start_symbol is not None:\n",
    "\n",
    "                    quarter_length_duration = step_duration * step_counter\n",
    "\n",
    "                    if start_symbol == \"r\":\n",
    "                        m21_event = m21.note.Rest(quarterLength=quarter_length_duration)\n",
    "\n",
    "                    else:\n",
    "                        m21_event = m21.note.Note(int(start_symbol), quarterLength=quarter_length_duration)\n",
    "\n",
    "                    stream.append(m21_event)\n",
    "\n",
    "                    step_counter = 1\n",
    "\n",
    "                start_symbol = symbol\n",
    "\n",
    "            # handle case in which we have a prolongation sign \"_\"\n",
    "            else:\n",
    "                step_counter += 1\n",
    "        stream = transpose_music_from_CA(stream, key, mode)\n",
    "\n",
    "        tempo_factor = 120 / tempo\n",
    "        \n",
    "        stream = stream.scaleOffsets(tempo_factor).scaleDurations(tempo_factor)\n",
    "        stream.write(format, file_name)\n",
    "\n",
    "\n",
    "def transpose_music_from_CA(score, key, mode):\n",
    "  if key == 'C' and mode== 'major' or key == 'A' and mode== 'minor':\n",
    "    return score\n",
    "\n",
    "  if mode=='major':\n",
    "    interval = m21.interval.Interval(m21.pitch.Pitch('C'), m21.pitch.Pitch(key))\n",
    "  elif mode=='minor':\n",
    "    interval = m21.interval.Interval(m21.pitch.Pitch('A'), m21.pitch.Pitch(key))\n",
    "  else:\n",
    "    interval = m21.interval.Interval(\"P1\")\n",
    "\n",
    "  return score.transpose(interval)\n",
    "\n",
    "\n",
    "def encode_midi_input(key, mode, midi_file):\n",
    "\n",
    "    raw_input_midi_seed = m21.converter.parse(midi_file) \n",
    "\n",
    "    seed_is_duration_complaint = check_durations(raw_input_midi_seed)\n",
    "    if(not seed_is_duration_complaint):\n",
    "        raise ValueError(\"Input MIDI file contains unsupported note/rest durations.\")\n",
    "\n",
    "    transposed_seed = transpose_music_to_CA(raw_input_midi_seed, key=key, mode=mode)\n",
    "    encoded_seed = encode_music(transposed_seed)\n",
    "\n",
    "    return encoded_seed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input variables\n",
    "key = \"C\"\n",
    "mode = \"mode\"\n",
    "temperature = 0.1\n",
    "tempo = 80\n",
    "midi_seed = 'test_melody_Cmin.mid'\n",
    "\n",
    "seed = encode_midi_input(key, mode, midi_seed)\n",
    "melody = generate_melody(seed, temperature=temperature)\n",
    "save_melody(seed.split(\" \"), file_name=\"prediction.mid\", tempo=tempo)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8135d194ff3aca28b844ad7a8ae0609e71165329dda20218c7abb1b797afa26e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
